{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2c939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, IntegerType,\n",
    "    StringType, DateType, DecimalType\n",
    ")\n",
    "\n",
    "from databricks.labs.dqx.engine import DQEngine\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a84f8",
   "metadata": {},
   "source": [
    "# Setting log level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd378f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74885188",
   "metadata": {},
   "source": [
    "# Create parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225943b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text('storage_account', '0')\n",
    "dbutils.widgets.text('year', '0')\n",
    "dbutils.widgets.text('month', '0')\n",
    "dbutils.widgets.text('day', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_account = dbutils.widgets.get('storage_account')\n",
    "year = dbutils.widgets.get('year')\n",
    "month = dbutils.widgets.get('month')\n",
    "day = dbutils.widgets.get('day')\n",
    "\n",
    "silver_file_path = f'abfss://silver@{storage_account}.dfs.core.windows.net/transformed_data/{year}/{month}/{day}/'\n",
    "\n",
    "# create data quality instance connected to databricks workspace\n",
    "dq_engine = DQEngine(WorkspaceClient())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da040de1",
   "metadata": {},
   "source": [
    "# Define schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f136ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_schema = StructType([\n",
    "    StructField('Sales_Person_ID', IntegerType(), False),\n",
    "    StructField('Sales_Person', StringType(), False),\n",
    "    StructField('Country', StringType(), False),\n",
    "    StructField('Product_ID', IntegerType(), False),\n",
    "    StructField('Product', StringType(), False),\n",
    "    StructField('Date', DateType(), False),\n",
    "    StructField('Revenue', IntegerType(), False),\n",
    "    StructField('Boxes_Shipped', IntegerType(), False),\n",
    "    StructField('First_Name', StringType(), False),\n",
    "    StructField('Last_Name', StringType(), False),\n",
    "    StructField('Revenue_Per_Box', DecimalType(10, 2), False),\n",
    "    StructField('Date_Key', StringType(), False),\n",
    "    StructField('Year', IntegerType(), False),\n",
    "    StructField('Quarter', IntegerType(), False),\n",
    "    StructField('Month', IntegerType(), False),\n",
    "    StructField('Day', IntegerType(), False),\n",
    "    StructField('Start_Of_Year', DateType(), False),\n",
    "    StructField('Start_Of_Quarter', DateType(), False),\n",
    "    StructField('Start_Of_Month', DateType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7cb53",
   "metadata": {},
   "source": [
    "# Run common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./utils/common_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4249f",
   "metadata": {},
   "source": [
    "# Define dimension functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_sales_person(\n",
    "    spark: SparkSession,\n",
    "    silver_schema: StructType,\n",
    "    silver_file_path: str,\n",
    "    dq_engine: DQEngine,\n",
    "    storage_account: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create sales person dimension table.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        silver_schema: Schema of silver layer data.\n",
    "        silver_file_path: File path in silver layer storage.\n",
    "        dq_engine: Data quality instance.\n",
    "        storage_account: Storage account name.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    dim_table_name = 'sales_catalog.gold.dim_sales_person'\n",
    "    checks_file_path = '/pipeline_project/check/checks_gold_dim_sales_person.yml'\n",
    "    merge_condition = 'trg.Sales_Person_Key = src.Sales_Person_Key'\n",
    "    gold_storage_path = f'abfss://gold@{storage_account}.dfs.core.windows.net/dim_sales_person'\n",
    "\n",
    "    # read data from silver layer\n",
    "    df_silver = read_data(spark, 'parquet', silver_schema, silver_file_path)\n",
    "\n",
    "    # select necessary columns\n",
    "    df_silver = df_silver.select(\n",
    "                                'Sales_Person_ID',\n",
    "                                'Sales_Person',\n",
    "                                'First_Name',\n",
    "                                'Last_Name'\n",
    "                            ).distinct()\n",
    "\n",
    "    # select dimension table data, select empty table if not existed\n",
    "    df_dimension = read_dim_data(\n",
    "                        spark,\n",
    "                        dim_table_name,\n",
    "                        'Sales_Person_Key, Sales_Person_ID, Sales_Person, First_Name, Last_Name',\n",
    "                        silver_file_path\n",
    "                    )\n",
    "    \n",
    "    # filter new records and existing records\n",
    "    df_all = df_silver.join(df_dimension, on='Sales_Person_ID', how='left') \\\n",
    "                        .select(\n",
    "                            df_dimension['Sales_Person_Key'],\n",
    "                            df_silver['Sales_Person_ID'],\n",
    "                            df_silver['Sales_Person'],\n",
    "                            df_silver['First_Name'],\n",
    "                            df_silver['Last_Name']\n",
    "                        )\n",
    "    \n",
    "    df_existed = df_all.filter(F.col('Sales_Person_Key').isNotNull())\n",
    "\n",
    "    df_new = df_all.filter(F.col('Sales_Person_Key').isNull())\n",
    "\n",
    "    # get maximum surrogate key\n",
    "    max_key = get_surrogate_key(spark, dim_table_name, df_dimension, 'Sales_Person_Key')\n",
    "\n",
    "    # generate surrogate key for new records\n",
    "    df_new = df_new.withColumn('Sales_Person_Key', F.monotonically_increasing_id() + max_key)\n",
    "\n",
    "    # combine new records with surrogate key and existing records\n",
    "    df_total = df_existed.union(df_new)\n",
    "\n",
    "    # data quality checks\n",
    "    data_quality_checks(dq_engine, checks_file_path, df_total)\n",
    "\n",
    "    # merge data\n",
    "    merge_data(spark, dim_table_name, df_total, merge_condition, gold_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d859291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_product(\n",
    "    spark: SparkSession,\n",
    "    silver_schema: StructType,\n",
    "    silver_file_path: str,\n",
    "    dq_engine: DQEngine,\n",
    "    storage_account: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create product dimension table.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        silver_schema: Schema of silver layer data.\n",
    "        silver_file_path: File path in silver layer storage.\n",
    "        dq_engine: Data quality instance.\n",
    "        storage_account: Storage account name.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    dim_table_name = 'sales_catalog.gold.dim_product'\n",
    "    checks_file_path = '/pipeline_project/check/checks_gold_dim_product.yml'\n",
    "    merge_condition = 'trg.Product_Key = src.Product_Key'\n",
    "    gold_storage_path = f'abfss://gold@{storage_account}.dfs.core.windows.net/dim_product'\n",
    "\n",
    "    # read data from silver layer\n",
    "    df_silver = read_data(spark, 'parquet', silver_schema, silver_file_path)\n",
    "\n",
    "    # select necessary columns\n",
    "    df_silver = df_silver.select('Product_ID', 'Product').distinct()\n",
    "\n",
    "    # select dimension table data, select empty table if not existed\n",
    "    df_dimension = read_dim_data(\n",
    "                        spark,\n",
    "                        dim_table_name,\n",
    "                        'Product_Key, Product_ID, Product',\n",
    "                        silver_file_path\n",
    "                    )\n",
    "    \n",
    "    # filter new records and existing records\n",
    "    df_all = df_silver.join(df_dimension, on='Product_ID', how='left') \\\n",
    "                        .select(\n",
    "                            df_dimension['Product_Key'],\n",
    "                            df_silver['Product_ID'],\n",
    "                            df_silver['Product']\n",
    "                        )\n",
    "    \n",
    "    df_existed = df_all.filter(F.col('Product_Key').isNotNull())\n",
    "\n",
    "    df_new = df_all.filter(F.col('Product_Key').isNull())\n",
    "\n",
    "    # get maximum surrogate key\n",
    "    max_key = get_surrogate_key(spark, dim_table_name, df_dimension, 'Product_Key')\n",
    "\n",
    "    # generate surrogate key for new records\n",
    "    df_new = df_new.withColumn('Product_Key', F.monotonically_increasing_id() + max_key)\n",
    "\n",
    "    # combine new records with surrogate key and existing records\n",
    "    df_total = df_existed.union(df_new)\n",
    "\n",
    "    # data quality checks\n",
    "    data_quality_checks(dq_engine, checks_file_path, df_total)\n",
    "\n",
    "    # merge data\n",
    "    merge_data(spark, dim_table_name, df_total, merge_condition, gold_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_country(\n",
    "    spark: SparkSession,\n",
    "    silver_schema: StructType,\n",
    "    silver_file_path: str,\n",
    "    dq_engine: DQEngine,\n",
    "    storage_account: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create country dimension table.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        silver_schema: Schema of silver layer data.\n",
    "        silver_file_path: File path in silver layer storage.\n",
    "        dq_engine: Data quality instance.\n",
    "        storage_account: Storage account name.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    dim_table_name = 'sales_catalog.gold.dim_country'\n",
    "    checks_file_path = '/pipeline_project/check/checks_gold_dim_country.yml'\n",
    "    merge_condition = 'trg.Country_Key = src.Country_Key'\n",
    "    gold_storage_path = f'abfss://gold@{storage_account}.dfs.core.windows.net/dim_country'\n",
    "\n",
    "    # read data from silver layer\n",
    "    df_silver = read_data(spark, 'parquet', silver_schema, silver_file_path)\n",
    "\n",
    "    # select necessary columns\n",
    "    df_silver = df_silver.select('Country').distinct()\n",
    "\n",
    "    # select dimension table data, select empty table if not existed\n",
    "    df_dimension = read_dim_data(\n",
    "                        spark,\n",
    "                        dim_table_name,\n",
    "                        'Country_Key, Country',\n",
    "                        silver_file_path\n",
    "                    )\n",
    "    \n",
    "    # filter new records and existing records\n",
    "    df_all = df_silver.join(df_dimension, on='Country', how='left') \\\n",
    "                        .select(\n",
    "                            df_dimension['Country_Key'],\n",
    "                            df_silver['Country']\n",
    "                        )\n",
    "    \n",
    "    df_existed = df_all.filter(F.col('Country_Key').isNotNull())\n",
    "\n",
    "    df_new = df_all.filter(F.col('Country_Key').isNull())\n",
    "\n",
    "    # get maximum surrogate key\n",
    "    max_key = get_surrogate_key(spark, dim_table_name, df_dimension, 'Country_Key')\n",
    "\n",
    "    # generate surrogate key for new records\n",
    "    df_new = df_new.withColumn('Country_Key', F.monotonically_increasing_id() + max_key)\n",
    "\n",
    "    # combine new records with surrogate key and existing records\n",
    "    df_total = df_existed.union(df_new)\n",
    "\n",
    "    # data quality checks\n",
    "    data_quality_checks(dq_engine, checks_file_path, df_total)\n",
    "\n",
    "    # merge data\n",
    "    merge_data(spark, dim_table_name, df_total, merge_condition, gold_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc28031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_date(\n",
    "    spark: SparkSession,\n",
    "    silver_schema: StructType,\n",
    "    silver_file_path: str,\n",
    "    dq_engine: DQEngine,\n",
    "    storage_account: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create date dimension table.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        silver_schema: Schema of silver layer data.\n",
    "        silver_file_path: File path in silver layer storage.\n",
    "        dq_engine: Data quality instance.\n",
    "        storage_account: Storage account name.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # define variables\n",
    "    dim_table_name = 'sales_catalog.gold.dim_date'\n",
    "    checks_file_path = '/pipeline_project/check/checks_gold_dim_date.yml'\n",
    "    merge_condition = 'trg.Date_Key = src.Date_Key'\n",
    "    gold_storage_path = f'abfss://gold@{storage_account}.dfs.core.windows.net/dim_date'\n",
    "\n",
    "    # read data from silver layer\n",
    "    df_silver = read_data(spark, 'parquet', silver_schema, silver_file_path)\n",
    "\n",
    "    # select necessary columns\n",
    "    df_total = df_silver.select(\n",
    "                            'Date_Key',\n",
    "                            'Date',\n",
    "                            'Year',\n",
    "                            'Quarter',\n",
    "                            'Month',\n",
    "                            'Day',\n",
    "                            'Start_Of_Year',\n",
    "                            'Start_Of_Quarter',\n",
    "                            'Start_Of_Month'\n",
    "                        ).distinct()\n",
    "\n",
    "    # data quality checks\n",
    "    data_quality_checks(dq_engine, checks_file_path, df_total)\n",
    "\n",
    "    # merge data\n",
    "    merge_data(spark, dim_table_name, df_total, merge_condition, gold_storage_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5852f23",
   "metadata": {},
   "source": [
    "# Define main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fd6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to create dimension tables.\n",
    "\n",
    "    Parameter:\n",
    "        None.\n",
    "    \n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        logging.info('Creating sales person dimension.')\n",
    "        create_dim_sales_person(spark, silver_schema, silver_file_path, dq_engine, storage_account)\n",
    "\n",
    "        logging.info('Creating product dimension.')\n",
    "        create_dim_product(spark, silver_schema, silver_file_path, dq_engine, storage_account)\n",
    "\n",
    "        logging.info('Creating country dimension.')\n",
    "        create_dim_country(spark, silver_schema, silver_file_path, dq_engine, storage_account)\n",
    "\n",
    "        logging.info('Creating date dimension.')\n",
    "        create_dim_date(spark, silver_schema, silver_file_path, dq_engine, storage_account)\n",
    "\n",
    "        logging.info('Finished creating all dimension tables.')\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error occured: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc87f6",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c503e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
