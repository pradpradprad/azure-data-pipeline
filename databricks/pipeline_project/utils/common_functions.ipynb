{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f25f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from databricks.labs.dqx.engine import DQEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a9633",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(spark: SparkSession, file_format: str, schema: StructType, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from data lake storage.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        file_format: File format name.\n",
    "        schema: Dataframe schema.\n",
    "        file_path: File path in storage.\n",
    "\n",
    "    Return:\n",
    "        Data as dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark \\\n",
    "            .read \\\n",
    "            .format(file_format) \\\n",
    "            .schema(schema) \\\n",
    "            .load(file_path)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c0af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dim_data(spark: SparkSession, dim_table: str, col: str, file_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from dimension table if existed. If not, select empty table from silver data.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        dim_table: Dimension table name.\n",
    "        col: Necessary columns.\n",
    "        file_path: Silver data file path to select columns from.\n",
    "    \n",
    "    Return:\n",
    "        Dimension table data as dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # if table existed (incremental load)\n",
    "    if spark.catalog.tableExists(dim_table):\n",
    "\n",
    "        df_dimension = spark.sql(f'''\n",
    "                                SELECT {col}\n",
    "                                FROM {dim_table}\n",
    "                            ''')\n",
    "    \n",
    "    # if not existed (full load)\n",
    "    else:\n",
    "\n",
    "        query = f'''\n",
    "            SELECT 1 AS {col}\n",
    "            FROM parquet.`{file_path}`\n",
    "            WHERE 1 = 0\n",
    "        '''\n",
    "        df_dimension = spark.sql(query)\n",
    "\n",
    "    return df_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca54aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrogate_key(spark: SparkSession, dim_table: str, df_dimension: DataFrame, col: str) -> int:\n",
    "    \"\"\"\n",
    "    Get maximum surrogate key before generating new key for new records.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark session.\n",
    "        dim_table: Dimension table name.\n",
    "        df_dimension: Dimension table dataframe.\n",
    "        col: Surrogate key column.\n",
    "\n",
    "    Return:\n",
    "        Maximum surrogate key as integer.\n",
    "    \"\"\"\n",
    "\n",
    "    # if table existed, get max key from column\n",
    "    if spark.catalog.tableExists(dim_table):\n",
    "\n",
    "        max_key = (df_dimension.select(F.max(col)).collect()[0][0] or 0) + 1\n",
    "\n",
    "    # if not existed, use 1 as starter value\n",
    "    else:\n",
    "\n",
    "        max_key = 1\n",
    "\n",
    "    return max_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_checks(dq_engine: DQEngine, checks_file_path: str, df: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Run data quality checks in dataframe using checks file, raise error if check failed.\n",
    "\n",
    "    Parameter:\n",
    "        dq_engine: Data quality instance.\n",
    "        checks_file_path: Workspace file path for checks file.\n",
    "        df: Dataframe to check data quality.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # load checks file\n",
    "    checks = dq_engine.load_checks_from_workspace_file(workspace_path=checks_file_path)\n",
    "\n",
    "    # apply checks to dataframe and return checks dataframe\n",
    "    df_check = dq_engine.apply_checks_by_metadata(df, checks)\n",
    "\n",
    "    # count checks error\n",
    "    error_count = df_check.select('_errors').filter(F.col('_errors').isNotNull()).count()\n",
    "\n",
    "    # raise error if failed\n",
    "    assert error_count == 0, f'{error_count} errors found in the data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b931da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(\n",
    "    spark: SparkSession,\n",
    "    table_name: str,\n",
    "    source_df: DataFrame,\n",
    "    merge_condition: str,\n",
    "    storage_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upsert data into delta table, create new table if not existed.\n",
    "\n",
    "    Parameter:\n",
    "        spark: Spark Session.\n",
    "        table_name: Delta table name.\n",
    "        source_df: Source dataframe to merge into dimension table.\n",
    "        merge_condition: Key condition to merge between source and target.\n",
    "        storage_path: Storage path to write data into.\n",
    "\n",
    "    Return:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # merge data into existing table\n",
    "    if spark.catalog.tableExists(table_name):\n",
    "\n",
    "        delta_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "        delta_table.alias('trg').merge(\n",
    "            source=source_df.alias('src'),\n",
    "            condition=merge_condition\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll() \\\n",
    "        .execute()\n",
    "\n",
    "    # create new table if not existed\n",
    "    else:\n",
    "\n",
    "        source_df \\\n",
    "            .write \\\n",
    "            .format('delta') \\\n",
    "            .mode('overwrite') \\\n",
    "            .option('path', storage_path) \\\n",
    "            .saveAsTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
